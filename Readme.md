### 人工神经网络ANN

#### 1、神经网络回归**

基本要求：

- 使用sklearn 的 MLP 实现神经网络
- 数据要求：

- - 回归问题：

- - - 选取任意标准数据集测试

- - 分类问题：

- - - 随机样本2分类
    - 选取任意标准数据集测试

提高练习：

- 尝试使用autograd实现梯度下降**【必修】**
- 尝试使用pytorch实现梯度下降（先手动update，再使用torch.optim模块）

- (有独显者)尝试使用 cuda ，对比GPU与CPU的运行效率
- 尝试手动实现梯度下降（难）

提高练习（应用类）：

- 试一试CNN人脸识别
- 试一试LSTM文本识别
- 试一试GAN（AI换脸，目前基本使用tensor flow)

#### 2、注意事项

1、`work.ipynb`为作业内容

2、`Derivation.ipynb`为手推ANN内容-只使用了3层神经网络（1层隐藏层）

3、数据集 -训练集：optdigits.tra；测试集：optdigits.tes（来源见ipynb文件）

3、实验总结

(1)、使用`sklearn`的MLP实现神经网络比较简单，直接调包即可。分别使用了-----`lbfgs`：quasi-Newton方法的优化器
\- `sgd`：随机梯度下降
\- `adam`： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器

发现随机梯度下降的迭代次数较大，准确率也比其他方法差

(2)、再手推`ANN`，遇到了较大困难，即便使用了参考书也依然难以实现较高的准确率，损失值下降到3000左右就停止了，学习率和迭代次数均有调试，仍未解决，原因不明。

