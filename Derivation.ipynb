{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 人工神经网络分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ANNClassifier:\n",
    "    def __init__(self, hidden_layer_sizes=(30, 30), eta=0.01,max_iter=500, tol=0.001):\n",
    "        '''构造器'''\n",
    "        # 各隐藏层节点个数\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        # 随机梯度下降学习率\n",
    "        self.eta = eta\n",
    "        # 随机梯度下降最大迭代次数\n",
    "        self.max_iter = max_iter\n",
    "        # 误差阈值\n",
    "        self.tol = tol\n",
    "    def _sigmoid(self, z):\n",
    "        '''激活函数，计算节点输出'''\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def _z(self, x, W):\n",
    "        '''加权求和，计算节点净输入'''\n",
    "        return x @ W\n",
    "    \n",
    "    def _error(self, y, y_pred):\n",
    "        '''计算误差'''\n",
    "        return np.sum((y - y_pred)**2)\n",
    "    \n",
    "    def _backpropagation(self, X, y):\n",
    "        '''基于梯度下降反向传播'''\n",
    "        \n",
    "        m, n = X.shape\n",
    "        _, n_out = y.shape\n",
    "        \n",
    "        #获得各层节点个数元组layer_sizes以及总数layer_n\n",
    "        layer_sizes = self.hidden_layer_sizes + (n_out,)\n",
    "        layer_n = len(layer_sizes)\n",
    "        \n",
    "        # 对于每一层, 将所有节点的权向量（以列向量的形式存为一个矩阵）,保存为W_list\n",
    "        W_list = []\n",
    "        li_size = n\n",
    "        for lj_size in layer_sizes:\n",
    "            W = np.random.rand(li_size + 1, lj_size) * 0.05\n",
    "            W_list.append(W) # 初始权值矩阵\n",
    "            li_size = lj_size\n",
    "            \n",
    "        # 创建梯度下降时所使用的列表\n",
    "        in_list = [None] * layer_n\n",
    "        z_list = [None] * layer_n\n",
    "        out_list = [None] * layer_n\n",
    "        delta_list = [None] * layer_n\n",
    "        self.err_list = [] # 存储每次迭代的损失值\n",
    "        # 随机梯度下降\n",
    "        idx = np.arange(m)\n",
    "        for n in range(self.max_iter):\n",
    "            # 随机打乱训练集\n",
    "            np.random.shuffle(idx)\n",
    "            X, y = X[idx], y[idx]\n",
    "            for x, t in zip(X,y):\n",
    "                # 单个样本作为输入运行神经网络\n",
    "                out = x\n",
    "                for i in range(layer_n):\n",
    "                    # 第 i-1 层输出添加x0=1, 作为第i层输入\n",
    "                    in_ = np.ones(out.size + 1)\n",
    "                    in_[1:] = out\n",
    "                    # 计算第i层所有节点净输入\n",
    "                    z = self._z(in_, W_list[i])\n",
    "                    # 计算第i层各节点的输出值\n",
    "                    out = self._sigmoid(z)\n",
    "                    # 保存第i层节点输入，净输入， 输出\n",
    "                    in_list[i], z_list[i], out_list[i] = in_, z, out\n",
    "            # 反向传播计算各节点的delta\n",
    "            # 输出层\n",
    "            delta_list[-1] = out * (1.0 - out) * (t - out)\n",
    "            # 隐藏层\n",
    "            for i in range(layer_n - 2, -1, -1):\n",
    "                out_i, W_j, delta_j = out_list[i], W_list[i+1], delta_list[i+1]\n",
    "                delta_list[i] = out_i * (1.0 - out_i) * (W_j[1:] @ delta_j[:, None]).T[0]\n",
    "                \n",
    "            # 更新所有节点的权\n",
    "            for i in range(layer_n):\n",
    "                in_i, delta_i = in_list[i], delta_list[i]\n",
    "                W_list[i] += in_i[:, None] * delta_i * self.eta\n",
    "\n",
    "            # 计算训练误差\n",
    "            y_pred = self._predict(X, W_list)\n",
    "            err = self._error(y, y_pred)\n",
    "            self.err_list.append(err)\n",
    "            # print(\"权值矩阵\",W_list)\n",
    "            print(\"迭代次数: %s 损失值: %s\"%(n,err))\n",
    "            if err < self.tol:\n",
    "                break\n",
    "        self.n = n\n",
    "        # 返回训练好的权矩阵列表\n",
    "        return W_list\n",
    "    def _predict(self, X, W_list, return_bin=False):\n",
    "        '''预测内部接口'''\n",
    "        \n",
    "        layer_n = len(W_list)\n",
    "        \n",
    "        out = X\n",
    "        for i in range(layer_n):\n",
    "            # 第i-1层输出添加x0=1, 作为第i层输入\n",
    "            m, n = out.shape\n",
    "            in_ = np.ones((m , n + 1))\n",
    "            in_[:, 1:] = out\n",
    "            # 第i层节点的净输入\n",
    "            z = self._z(in_, W_list[i])\n",
    "            # 计算第i层所有节点输出值\n",
    "            out = self._sigmoid(z)\n",
    "        \n",
    "        # 是否返回二进制编码的类标记\n",
    "        if return_bin:\n",
    "            # 输出最大的节点输出编码为1, 其他节点输出编码为0\n",
    "            idx = np.argmax(out, axis=1)\n",
    "            out_bin = np.zeros_like(out)\n",
    "            out_bin[range(len(idx)), idx] = 1\n",
    "            return out_bin\n",
    "        return out\n",
    "    def train(self, X, y):\n",
    "        '''训练'''\n",
    "        # 调用反向传播算法训练神经网络中所有节点的权\n",
    "        self.W_list = self._backpropagation(X, y)\n",
    "    def predict(self, X):\n",
    "        '''预测'''\n",
    "        return self._predict(X, self.W_list, return_bin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调用Numpy.genfromtxt函数加载`手写数字`数据集\n",
    "\n",
    "训练集：https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
    "\n",
    "测试集：https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练集\n",
    "data_train = np.genfromtxt('optdigits.tra', delimiter=',', dtype=float)\n",
    "X_train,y_train = data_train[:, :-1], data_train[:, -1]\n",
    "# 测试集\n",
    "data_test = np.genfromtxt('optdigits.tes', delimiter=',', dtype=float)\n",
    "X_test, y_test = data_test[:, :-1], data_test[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用sklearn.preprocessing.StandardScaler对数据特征进行标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train) # 使用训练集进行拟合\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# 对训练集进行标准化\n",
    "X_train_std = ss.transform(X_train)\n",
    "#对测试集进行标准化\n",
    "X_test_std = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 对于分类问题，神经网络的每一个输出节点对于应一个类别，则数据类型标记是`1 of n`形式的二进制编码，而目前训练数据集中的类型标记是int类型的数字，因此使用sklearn.preprocessing.LabelBinarizer对y_train进行`1 of n`编码。后由于模型预测是预测值也是`1 of n`编码，因此后面也需要调用`lb.inverse_tranform`方法将它们解码成int类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型的训练和测试\n",
    "创建一个3层神经网络，输入层和输出层分别有64个节点和10个节点（由训练集64个特征和10个类别决定）；隐藏层结构包含100个节点（自行定义）\n",
    " - `hidden_layer_sizes` = (100,) 隐藏层为1层100个节点\n",
    " - `eta`=0.05 随机梯度下降的学习率\n",
    " - `max_iter`=500 迭代次数为500\n",
    " - `tol`=0.00001 判断收敛的误差阈值为0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数: 0 损失值: 11504.024079043698\n",
      "迭代次数: 1 损失值: 7050.262731555939\n",
      "迭代次数: 2 损失值: 5298.600139242277\n",
      "迭代次数: 3 损失值: 4622.085114607979\n",
      "迭代次数: 4 损失值: 4491.54173973689\n",
      "迭代次数: 5 损失值: 3783.362343119701\n",
      "迭代次数: 6 损失值: 3617.3436327012128\n",
      "迭代次数: 7 损失值: 3546.1296855547703\n",
      "迭代次数: 8 损失值: 3756.681874613696\n",
      "迭代次数: 9 损失值: 3571.917789111574\n",
      "迭代次数: 10 损失值: 3502.213525465268\n",
      "迭代次数: 11 损失值: 3477.0404829509507\n",
      "迭代次数: 12 损失值: 3609.5145129321645\n",
      "迭代次数: 13 损失值: 3511.3115121526344\n",
      "迭代次数: 14 损失值: 3885.2208501725677\n",
      "迭代次数: 15 损失值: 3545.7155549450144\n",
      "迭代次数: 16 损失值: 3539.6946418894095\n",
      "迭代次数: 17 损失值: 3493.458187534631\n",
      "迭代次数: 18 损失值: 3473.2058299419396\n",
      "迭代次数: 19 损失值: 3508.8956613993246\n",
      "迭代次数: 20 损失值: 3557.586948260371\n",
      "迭代次数: 21 损失值: 3521.1358914347134\n",
      "迭代次数: 22 损失值: 3767.3270447339555\n",
      "迭代次数: 23 损失值: 3536.287451546241\n",
      "迭代次数: 24 损失值: 3538.025209405739\n",
      "迭代次数: 25 损失值: 3494.024096580437\n",
      "迭代次数: 26 损失值: 3480.459564854087\n",
      "迭代次数: 27 损失值: 3470.4946750852623\n",
      "迭代次数: 28 损失值: 3479.022383978339\n",
      "迭代次数: 29 损失值: 3486.1119795326786\n",
      "迭代次数: 30 损失值: 3474.8106807579575\n",
      "迭代次数: 31 损失值: 3470.9045882328264\n",
      "迭代次数: 32 损失值: 3475.099374403502\n",
      "迭代次数: 33 损失值: 3474.390052299716\n",
      "迭代次数: 34 损失值: 3526.8701869613324\n",
      "迭代次数: 35 损失值: 3493.573776918215\n",
      "迭代次数: 36 损失值: 3695.0335936330584\n",
      "迭代次数: 37 损失值: 4416.57327543118\n",
      "迭代次数: 38 损失值: 3603.2514344111883\n",
      "迭代次数: 39 损失值: 3519.534396457145\n",
      "迭代次数: 40 损失值: 3855.7638812006717\n",
      "迭代次数: 41 损失值: 3550.0298612359134\n",
      "迭代次数: 42 损失值: 3507.804325046765\n",
      "迭代次数: 43 损失值: 3492.1023051507805\n",
      "迭代次数: 44 损失值: 3509.6486685135383\n",
      "迭代次数: 45 损失值: 3509.067738181736\n",
      "迭代次数: 46 损失值: 3494.672291645731\n",
      "迭代次数: 47 损失值: 3486.2322453773777\n",
      "迭代次数: 48 损失值: 3491.192278687073\n",
      "迭代次数: 49 损失值: 3485.03534038121\n",
      "迭代次数: 50 损失值: 3480.61389169792\n",
      "迭代次数: 51 损失值: 3488.2026958136125\n",
      "迭代次数: 52 损失值: 3485.1381026740755\n",
      "迭代次数: 53 损失值: 3485.221695562569\n",
      "迭代次数: 54 损失值: 3480.6734693908193\n",
      "迭代次数: 55 损失值: 3485.5594340172815\n",
      "迭代次数: 56 损失值: 3515.609455101673\n",
      "迭代次数: 57 损失值: 3494.208726547723\n",
      "迭代次数: 58 损失值: 3484.078186120909\n",
      "迭代次数: 59 损失值: 3508.6906580639343\n",
      "迭代次数: 60 损失值: 3555.2609997263253\n",
      "迭代次数: 61 损失值: 3515.232368107663\n",
      "迭代次数: 62 损失值: 3584.315523046105\n",
      "迭代次数: 63 损失值: 3525.059181238479\n",
      "迭代次数: 64 损失值: 3506.1064584461687\n",
      "迭代次数: 65 损失值: 3492.330969150076\n",
      "迭代次数: 66 损失值: 3500.0554681057274\n",
      "迭代次数: 67 损失值: 3520.7545894069312\n",
      "迭代次数: 68 损失值: 3497.6853765148\n",
      "迭代次数: 69 损失值: 3492.335426539921\n",
      "迭代次数: 70 损失值: 3485.1358055132237\n",
      "迭代次数: 71 损失值: 3480.7332117541737\n",
      "迭代次数: 72 损失值: 3477.0755894200506\n",
      "迭代次数: 73 损失值: 3475.0940013605496\n",
      "迭代次数: 74 损失值: 3474.7148369302568\n",
      "迭代次数: 75 损失值: 3473.1258487578807\n",
      "迭代次数: 76 损失值: 3472.8628843899633\n",
      "迭代次数: 77 损失值: 3477.4463832765996\n",
      "迭代次数: 78 损失值: 3475.357800597375\n",
      "迭代次数: 79 损失值: 3476.318740646652\n",
      "迭代次数: 80 损失值: 3478.1779811963142\n",
      "迭代次数: 81 损失值: 3498.47206237722\n",
      "迭代次数: 82 损失值: 3487.2327990450585\n",
      "迭代次数: 83 损失值: 3503.228333004896\n",
      "迭代次数: 84 损失值: 3489.3891671211236\n",
      "迭代次数: 85 损失值: 3483.5810990687887\n",
      "迭代次数: 86 损失值: 3478.4607557651757\n",
      "迭代次数: 87 损失值: 3486.296431617243\n",
      "迭代次数: 88 损失值: 3484.9634072888184\n",
      "迭代次数: 89 损失值: 3539.813528322654\n",
      "迭代次数: 90 损失值: 3518.243128114445\n",
      "迭代次数: 91 损失值: 3497.841871089241\n",
      "迭代次数: 92 损失值: 3489.246420380059\n",
      "迭代次数: 93 损失值: 3484.861427819474\n",
      "迭代次数: 94 损失值: 3482.0016589473603\n",
      "迭代次数: 95 损失值: 3482.313923707206\n",
      "迭代次数: 96 损失值: 3522.771525900698\n",
      "迭代次数: 97 损失值: 3498.8926392230737\n",
      "迭代次数: 98 损失值: 3504.8122314043635\n",
      "迭代次数: 99 损失值: 3499.2467109059926\n",
      "迭代次数: 100 损失值: 3485.2009340323298\n",
      "迭代次数: 101 损失值: 3478.1526770167866\n",
      "迭代次数: 102 损失值: 3514.8604403895943\n",
      "迭代次数: 103 损失值: 3827.0464494147227\n",
      "迭代次数: 104 损失值: 3556.7777326710516\n",
      "迭代次数: 105 损失值: 3505.5378290178596\n",
      "迭代次数: 106 损失值: 3490.4276622524894\n",
      "迭代次数: 107 损失值: 3481.3957437397203\n",
      "迭代次数: 108 损失值: 3479.9677231528385\n",
      "迭代次数: 109 损失值: 3527.8565543018267\n",
      "迭代次数: 110 损失值: 3498.236198697806\n",
      "迭代次数: 111 损失值: 3495.8723760688595\n",
      "迭代次数: 112 损失值: 3503.1582266039904\n",
      "迭代次数: 113 损失值: 3506.861531818347\n",
      "迭代次数: 114 损失值: 3490.295033539006\n",
      "迭代次数: 115 损失值: 3501.9101500807183\n",
      "迭代次数: 116 损失值: 3548.7808558176193\n",
      "迭代次数: 117 损失值: 3509.1371353712543\n",
      "迭代次数: 118 损失值: 3492.5041534782845\n",
      "迭代次数: 119 损失值: 3484.219454613842\n",
      "迭代次数: 120 损失值: 3488.2939733267585\n",
      "迭代次数: 121 损失值: 3485.278631375272\n",
      "迭代次数: 122 损失值: 3477.944841145354\n",
      "迭代次数: 123 损失值: 3498.700004909779\n",
      "迭代次数: 124 损失值: 3486.4283346691136\n",
      "迭代次数: 125 损失值: 3512.185567134401\n",
      "迭代次数: 126 损失值: 3488.66267044134\n",
      "迭代次数: 127 损失值: 3479.6501077732037\n",
      "迭代次数: 128 损失值: 3484.460743298571\n",
      "迭代次数: 129 损失值: 3478.136204330976\n",
      "迭代次数: 130 损失值: 3536.3624116865744\n",
      "迭代次数: 131 损失值: 3500.697255388756\n",
      "迭代次数: 132 损失值: 3485.6354724590556\n",
      "迭代次数: 133 损失值: 3479.240936965658\n",
      "迭代次数: 134 损失值: 3476.7294363755213\n",
      "迭代次数: 135 损失值: 3473.7345695297936\n",
      "迭代次数: 136 损失值: 3471.397916863105\n",
      "迭代次数: 137 损失值: 3478.342372976691\n",
      "迭代次数: 138 损失值: 3477.167848915861\n",
      "迭代次数: 139 损失值: 3474.3128739572603\n",
      "迭代次数: 140 损失值: 3469.272811755894\n",
      "迭代次数: 141 损失值: 3466.2287229535677\n",
      "迭代次数: 142 损失值: 3473.5963538744727\n",
      "迭代次数: 143 损失值: 3469.7794866061677\n",
      "迭代次数: 144 损失值: 3475.3051688979563\n",
      "迭代次数: 145 损失值: 3470.244003773181\n",
      "迭代次数: 146 损失值: 3468.3513628595074\n",
      "迭代次数: 147 损失值: 3465.708838255163\n",
      "迭代次数: 148 损失值: 3475.9452496557096\n",
      "迭代次数: 149 损失值: 3485.9497089991933\n",
      "迭代次数: 150 损失值: 3495.0605603219183\n",
      "迭代次数: 151 损失值: 3479.295332334012\n",
      "迭代次数: 152 损失值: 3579.892953349378\n",
      "迭代次数: 153 损失值: 3506.1266075673047\n",
      "迭代次数: 154 损失值: 3482.572613727414\n",
      "迭代次数: 155 损失值: 3472.780304350563\n",
      "迭代次数: 156 损失值: 3480.564649687167\n",
      "迭代次数: 157 损失值: 3469.6674034541143\n",
      "迭代次数: 158 损失值: 3465.061867163602\n",
      "迭代次数: 159 损失值: 3464.475132870231\n",
      "迭代次数: 160 损失值: 3461.0649391133875\n",
      "迭代次数: 161 损失值: 3492.660166258402\n",
      "迭代次数: 162 损失值: 3478.803727011091\n",
      "迭代次数: 163 损失值: 3611.4026550053154\n",
      "迭代次数: 164 损失值: 3518.0958132727833\n",
      "迭代次数: 165 损失值: 3496.7836570183085\n",
      "迭代次数: 166 损失值: 3525.3877959924007\n",
      "迭代次数: 167 损失值: 3505.4912294066335\n",
      "迭代次数: 168 损失值: 3484.805309538039\n",
      "迭代次数: 169 损失值: 3547.099217142196\n",
      "迭代次数: 170 损失值: 3499.080345777941\n",
      "迭代次数: 171 损失值: 3480.1543376934987\n",
      "迭代次数: 172 损失值: 3509.149902218057\n",
      "迭代次数: 173 损失值: 3494.7966060209537\n",
      "迭代次数: 174 损失值: 3479.1902254078705\n",
      "迭代次数: 175 损失值: 3470.290445108508\n",
      "迭代次数: 176 损失值: 3464.9706095826177\n",
      "迭代次数: 177 损失值: 3461.0124624772006\n",
      "迭代次数: 178 损失值: 3459.3962587220494\n",
      "迭代次数: 179 损失值: 3457.084486434673\n",
      "迭代次数: 180 损失值: 3458.0484052868946\n",
      "迭代次数: 181 损失值: 3455.191942349509\n",
      "迭代次数: 182 损失值: 3478.4909125709914\n",
      "迭代次数: 183 损失值: 3482.7485323806563\n",
      "迭代次数: 184 损失值: 3596.059043865544\n",
      "迭代次数: 185 损失值: 3511.175873544762\n",
      "迭代次数: 186 损失值: 3478.4501085948564\n",
      "迭代次数: 187 损失值: 3468.834709480374\n",
      "迭代次数: 188 损失值: 3464.7860379944786\n",
      "迭代次数: 189 损失值: 3459.9213322286164\n",
      "迭代次数: 190 损失值: 3473.2319364525392\n",
      "迭代次数: 191 损失值: 3467.2545756864934\n",
      "迭代次数: 192 损失值: 3462.097375604563\n",
      "迭代次数: 193 损失值: 3457.6069131357685\n",
      "迭代次数: 194 损失值: 3463.914302820478\n",
      "迭代次数: 195 损失值: 3461.1648968750014\n",
      "迭代次数: 196 损失值: 3478.7870808245134\n",
      "迭代次数: 197 损失值: 3476.470862960125\n",
      "迭代次数: 198 损失值: 3464.02903001874\n",
      "迭代次数: 199 损失值: 3458.2453669603456\n",
      "迭代次数: 200 损失值: 3454.6281590499916\n",
      "迭代次数: 201 损失值: 3458.9547157978814\n",
      "迭代次数: 202 损失值: 3453.046774308135\n",
      "迭代次数: 203 损失值: 3501.595073203244\n",
      "迭代次数: 204 损失值: 3477.3910497680895\n",
      "迭代次数: 205 损失值: 3461.518886793623\n",
      "迭代次数: 206 损失值: 3463.747719055185\n",
      "迭代次数: 207 损失值: 3456.3134955785\n",
      "迭代次数: 208 损失值: 3470.1833605193137\n",
      "迭代次数: 209 损失值: 3461.5804826247263\n",
      "迭代次数: 210 损失值: 3469.452920744529\n",
      "迭代次数: 211 损失值: 3476.8718670725334\n",
      "迭代次数: 212 损失值: 3464.7184712084418\n",
      "迭代次数: 213 损失值: 3471.412831030199\n",
      "迭代次数: 214 损失值: 3517.85617891044\n",
      "迭代次数: 215 损失值: 3479.440668671697\n",
      "迭代次数: 216 损失值: 3466.3170067960436\n",
      "迭代次数: 217 损失值: 3458.838788706164\n",
      "迭代次数: 218 损失值: 3465.1962067832783\n",
      "迭代次数: 219 损失值: 3462.954059144464\n",
      "迭代次数: 220 损失值: 3458.2028424480027\n",
      "迭代次数: 221 损失值: 3526.6208469070107\n",
      "迭代次数: 222 损失值: 3509.8241071850252\n",
      "迭代次数: 223 损失值: 3473.3965770343034\n",
      "迭代次数: 224 损失值: 3466.9685395538313\n",
      "迭代次数: 225 损失值: 3456.5321054004507\n",
      "迭代次数: 226 损失值: 3523.7990580897285\n",
      "迭代次数: 227 损失值: 3483.4504699152544\n",
      "迭代次数: 228 损失值: 3800.513268223452\n",
      "迭代次数: 229 损失值: 3518.5436811928666\n",
      "迭代次数: 230 损失值: 3525.240079816707\n",
      "迭代次数: 231 损失值: 3488.6842923797144\n",
      "迭代次数: 232 损失值: 3479.0785994047415\n",
      "迭代次数: 233 损失值: 3465.8349806353817\n",
      "迭代次数: 234 损失值: 3483.4413683378434\n",
      "迭代次数: 235 损失值: 3516.563973309567\n",
      "迭代次数: 236 损失值: 3509.623441835804\n",
      "迭代次数: 237 损失值: 3729.08573347769\n",
      "迭代次数: 238 损失值: 3531.3291634697302\n",
      "迭代次数: 239 损失值: 3526.4918401634786\n",
      "迭代次数: 240 损失值: 3798.660858727386\n",
      "迭代次数: 241 损失值: 3532.0058509952064\n",
      "迭代次数: 242 损失值: 3489.749812651399\n",
      "迭代次数: 243 损失值: 3473.0602209098033\n",
      "迭代次数: 244 损失值: 3459.9599336288134\n",
      "迭代次数: 245 损失值: 3457.4489610148985\n",
      "迭代次数: 246 损失值: 3457.891636185277\n",
      "迭代次数: 247 损失值: 3459.289012010471\n",
      "迭代次数: 248 损失值: 3453.3319335588976\n",
      "迭代次数: 249 损失值: 3452.084192273128\n",
      "迭代次数: 250 损失值: 3451.6901823557255\n",
      "迭代次数: 251 损失值: 3451.3286034702915\n",
      "迭代次数: 252 损失值: 3457.747889100827\n",
      "迭代次数: 253 损失值: 3452.4786490713855\n",
      "迭代次数: 254 损失值: 3505.279133208471\n",
      "迭代次数: 255 损失值: 3474.596501804025\n",
      "迭代次数: 256 损失值: 3461.6772033759\n",
      "迭代次数: 257 损失值: 3465.2869826324286\n",
      "迭代次数: 258 损失值: 3456.5785507928044\n",
      "迭代次数: 259 损失值: 3452.125843447582\n",
      "迭代次数: 260 损失值: 3451.8096043977002\n",
      "迭代次数: 261 损失值: 3447.569321663135\n",
      "迭代次数: 262 损失值: 3456.882436062658\n",
      "迭代次数: 263 损失值: 3449.4866826891075\n",
      "迭代次数: 264 损失值: 3448.116548946745\n",
      "迭代次数: 265 损失值: 3490.2810812858415\n",
      "迭代次数: 266 损失值: 3504.972434781046\n",
      "迭代次数: 267 损失值: 3477.899813412779\n",
      "迭代次数: 268 损失值: 3574.296714444578\n",
      "迭代次数: 269 损失值: 3493.2922640080246\n",
      "迭代次数: 270 损失值: 3465.0758497244387\n",
      "迭代次数: 271 损失值: 3447.771745823502\n",
      "迭代次数: 272 损失值: 3441.56701362457\n",
      "迭代次数: 273 损失值: 3464.3393156616935\n",
      "迭代次数: 274 损失值: 3449.5009708015173\n",
      "迭代次数: 275 损失值: 3570.4502916388446\n",
      "迭代次数: 276 损失值: 3478.7116481738194\n",
      "迭代次数: 277 损失值: 3462.158199073947\n",
      "迭代次数: 278 损失值: 3454.2446534637324\n",
      "迭代次数: 279 损失值: 3449.01600911982\n",
      "迭代次数: 280 损失值: 3437.532023304778\n",
      "迭代次数: 281 损失值: 3459.510542670981\n",
      "迭代次数: 282 损失值: 3447.003390915393\n",
      "迭代次数: 283 损失值: 3446.8120564869528\n",
      "迭代次数: 284 损失值: 3503.592887108105\n",
      "迭代次数: 285 损失值: 3507.8466190283975\n",
      "迭代次数: 286 损失值: 3467.6801704030245\n",
      "迭代次数: 287 损失值: 3448.344063990774\n",
      "迭代次数: 288 损失值: 3457.8516022143303\n",
      "迭代次数: 289 损失值: 3447.5296873034413\n",
      "迭代次数: 290 损失值: 3437.589964405624\n",
      "迭代次数: 291 损失值: 3445.1717767259615\n",
      "迭代次数: 292 损失值: 3441.3061687579275\n",
      "迭代次数: 293 损失值: 3438.7725183983403\n",
      "迭代次数: 294 损失值: 3436.6557431667866\n",
      "迭代次数: 295 损失值: 3479.2369767831033\n",
      "迭代次数: 296 损失值: 3459.6265116566783\n",
      "迭代次数: 297 损失值: 3451.569635595005\n",
      "迭代次数: 298 损失值: 3443.955555044816\n",
      "迭代次数: 299 损失值: 3470.1852171295986\n",
      "迭代次数: 300 损失值: 3504.5236918383885\n",
      "迭代次数: 301 损失值: 3634.111785273456\n",
      "迭代次数: 302 损失值: 3721.411061051135\n",
      "迭代次数: 303 损失值: 3520.2520519850386\n",
      "迭代次数: 304 损失值: 3466.979115323414\n",
      "迭代次数: 305 损失值: 3578.47473363941\n",
      "迭代次数: 306 损失值: 4175.937159098972\n",
      "迭代次数: 307 损失值: 3523.964077705671\n",
      "迭代次数: 308 损失值: 3469.829999458675\n",
      "迭代次数: 309 损失值: 3753.1009591896195\n",
      "迭代次数: 310 损失值: 3518.776728107195\n",
      "迭代次数: 311 损失值: 3466.6972018929127\n",
      "迭代次数: 312 损失值: 3810.6848840021044\n",
      "迭代次数: 313 损失值: 3551.6190487753342\n",
      "迭代次数: 314 损失值: 3481.689716441077\n",
      "迭代次数: 315 损失值: 3449.4912177957217\n",
      "迭代次数: 316 损失值: 3440.6870873413686\n",
      "迭代次数: 317 损失值: 3436.71442313265\n",
      "迭代次数: 318 损失值: 3500.852817146745\n",
      "迭代次数: 319 损失值: 3449.9672410689423\n",
      "迭代次数: 320 损失值: 3461.96724587989\n",
      "迭代次数: 321 损失值: 3730.785575112553\n",
      "迭代次数: 322 损失值: 3484.6496024163607\n",
      "迭代次数: 323 损失值: 3451.681810111422\n",
      "迭代次数: 324 损失值: 3705.4079806985255\n",
      "迭代次数: 325 损失值: 3500.95014599166\n",
      "迭代次数: 326 损失值: 3451.3107347428286\n",
      "迭代次数: 327 损失值: 3445.3302983981966\n",
      "迭代次数: 328 损失值: 3433.0675852335116\n",
      "迭代次数: 329 损失值: 3429.565328433871\n",
      "迭代次数: 330 损失值: 3428.5446415626875\n",
      "迭代次数: 331 损失值: 3425.74252014885\n",
      "迭代次数: 332 损失值: 3431.2071477303143\n",
      "迭代次数: 333 损失值: 3431.2123882773776\n",
      "迭代次数: 334 损失值: 3429.910927873229\n",
      "迭代次数: 335 损失值: 3423.3304181530157\n",
      "迭代次数: 336 损失值: 3439.414746056319\n",
      "迭代次数: 337 损失值: 3431.4583165906497\n",
      "迭代次数: 338 损失值: 3425.0660408067815\n",
      "迭代次数: 339 损失值: 3422.294767664395\n",
      "迭代次数: 340 损失值: 3450.3769025457855\n",
      "迭代次数: 341 损失值: 3434.7347640267362\n",
      "迭代次数: 342 损失值: 3454.30885976538\n",
      "迭代次数: 343 损失值: 3435.445478428121\n",
      "迭代次数: 344 损失值: 3435.254825642698\n",
      "迭代次数: 345 损失值: 3592.6399766713594\n",
      "迭代次数: 346 损失值: 3457.8219105046237\n",
      "迭代次数: 347 损失值: 3432.4016392357867\n",
      "迭代次数: 348 损失值: 3418.3507554980524\n",
      "迭代次数: 349 损失值: 3405.8630590620555\n",
      "迭代次数: 350 损失值: 3486.29756138026\n",
      "迭代次数: 351 损失值: 3438.3680646077823\n",
      "迭代次数: 352 损失值: 3418.0191836428057\n",
      "迭代次数: 353 损失值: 3404.503789408439\n",
      "迭代次数: 354 损失值: 3401.2624905473804\n",
      "迭代次数: 355 损失值: 3402.349477095506\n",
      "迭代次数: 356 损失值: 3399.7539528846373\n",
      "迭代次数: 357 损失值: 3408.768745063857\n",
      "迭代次数: 358 损失值: 3401.2370641082393\n",
      "迭代次数: 359 损失值: 3394.92116022203\n",
      "迭代次数: 360 损失值: 3392.164589540491\n",
      "迭代次数: 361 损失值: 3441.2697041020347\n",
      "迭代次数: 362 损失值: 3407.4273980944276\n",
      "迭代次数: 363 损失值: 3403.5594983392652\n",
      "迭代次数: 364 损失值: 3515.385713797009\n",
      "迭代次数: 365 损失值: 3447.461696687486\n",
      "迭代次数: 366 损失值: 3421.36095633873\n",
      "迭代次数: 367 损失值: 3500.8095505293445\n",
      "迭代次数: 368 损失值: 3431.058292877265\n",
      "迭代次数: 369 损失值: 3401.3903326144987\n",
      "迭代次数: 370 损失值: 3388.8155893582416\n",
      "迭代次数: 371 损失值: 3390.434268612114\n",
      "迭代次数: 372 损失值: 3406.7475805368936\n",
      "迭代次数: 373 损失值: 3671.9979408928534\n",
      "迭代次数: 374 损失值: 4518.87570364662\n",
      "迭代次数: 375 损失值: 3731.8891846790357\n",
      "迭代次数: 376 损失值: 4447.680401241974\n",
      "迭代次数: 377 损失值: 3696.4143951650385\n",
      "迭代次数: 378 损失值: 3486.6480623503476\n",
      "迭代次数: 379 损失值: 3418.6211826838207\n",
      "迭代次数: 380 损失值: 3399.3666491504196\n",
      "迭代次数: 381 损失值: 3440.229393552626\n",
      "迭代次数: 382 损失值: 3514.829624907499\n",
      "迭代次数: 383 损失值: 3443.9254192903145\n",
      "迭代次数: 384 损失值: 3406.6368936686404\n",
      "迭代次数: 385 损失值: 3395.599867006513\n",
      "迭代次数: 386 损失值: 3390.4946562618625\n",
      "迭代次数: 387 损失值: 3389.634904663175\n",
      "迭代次数: 388 损失值: 3420.669302498838\n",
      "迭代次数: 389 损失值: 3470.4358277311294\n",
      "迭代次数: 390 损失值: 3402.9446845638704\n",
      "迭代次数: 391 损失值: 3381.162715856404\n",
      "迭代次数: 392 损失值: 3380.4564505234466\n",
      "迭代次数: 393 损失值: 3372.1731710215017\n",
      "迭代次数: 394 损失值: 3370.2286763033662\n",
      "迭代次数: 395 损失值: 3388.6938039028323\n",
      "迭代次数: 396 损失值: 3410.0448639520214\n",
      "迭代次数: 397 损失值: 3411.58541019547\n",
      "迭代次数: 398 损失值: 3517.389890458123\n",
      "迭代次数: 399 损失值: 3739.438810111976\n",
      "迭代次数: 400 损失值: 3429.688848312114\n",
      "迭代次数: 401 损失值: 3834.637659596351\n",
      "迭代次数: 402 损失值: 3421.469749748164\n",
      "迭代次数: 403 损失值: 3378.7943249699533\n",
      "迭代次数: 404 损失值: 3373.5319042340757\n",
      "迭代次数: 405 损失值: 3359.41616746644\n",
      "迭代次数: 406 损失值: 3357.8239139458015\n",
      "迭代次数: 407 损失值: 3417.7679434152988\n",
      "迭代次数: 408 损失值: 3386.563867613246\n",
      "迭代次数: 409 损失值: 3373.931077180103\n",
      "迭代次数: 410 损失值: 3368.421421680173\n",
      "迭代次数: 411 损失值: 3357.2378217021806\n",
      "迭代次数: 412 损失值: 3367.269948581728\n",
      "迭代次数: 413 损失值: 3345.2116359024226\n",
      "迭代次数: 414 损失值: 3338.6981820504457\n",
      "迭代次数: 415 损失值: 3359.020252922988\n",
      "迭代次数: 416 损失值: 3407.4841011983362\n",
      "迭代次数: 417 损失值: 3358.7692322362873\n",
      "迭代次数: 418 损失值: 3347.4442741067323\n",
      "迭代次数: 419 损失值: 3342.7392015028527\n",
      "迭代次数: 420 损失值: 3342.7894519563015\n",
      "迭代次数: 421 损失值: 3346.1608533258027\n",
      "迭代次数: 422 损失值: 3347.1423429091565\n",
      "迭代次数: 423 损失值: 3348.1030738791624\n",
      "迭代次数: 424 损失值: 3346.116032422435\n",
      "迭代次数: 425 损失值: 3345.9733059810173\n",
      "迭代次数: 426 损失值: 3353.1756973208194\n",
      "迭代次数: 427 损失值: 3354.2136992145815\n",
      "迭代次数: 428 损失值: 3380.045410041148\n",
      "迭代次数: 429 损失值: 3360.550945775174\n",
      "迭代次数: 430 损失值: 3336.3444321721454\n",
      "迭代次数: 431 损失值: 3335.830186451387\n",
      "迭代次数: 432 损失值: 3385.138220902887\n",
      "迭代次数: 433 损失值: 3362.724030641845\n",
      "迭代次数: 434 损失值: 3349.3135460024523\n",
      "迭代次数: 435 损失值: 3373.382231601621\n",
      "迭代次数: 436 损失值: 3340.402853221739\n",
      "迭代次数: 437 损失值: 3332.850681973833\n",
      "迭代次数: 438 损失值: 3571.815939019316\n",
      "迭代次数: 439 损失值: 3358.9077771195853\n",
      "迭代次数: 440 损失值: 3335.57581210842\n",
      "迭代次数: 441 损失值: 3313.6245259451034\n",
      "迭代次数: 442 损失值: 3309.635669558207\n",
      "迭代次数: 443 损失值: 3285.882983565135\n",
      "迭代次数: 444 损失值: 3393.4373671404583\n",
      "迭代次数: 445 损失值: 3364.60568699542\n",
      "迭代次数: 446 损失值: 3374.823176821176\n",
      "迭代次数: 447 损失值: 3285.682776117728\n",
      "迭代次数: 448 损失值: 3270.9300845261564\n",
      "迭代次数: 449 损失值: 3265.9560061213915\n",
      "迭代次数: 450 损失值: 3304.3222749856122\n",
      "迭代次数: 451 损失值: 3274.5194409500245\n",
      "迭代次数: 452 损失值: 3255.9237901564397\n",
      "迭代次数: 453 损失值: 3295.604760711638\n",
      "迭代次数: 454 损失值: 3269.078402115329\n",
      "迭代次数: 455 损失值: 3256.796608220965\n",
      "迭代次数: 456 损失值: 3246.4025166563692\n",
      "迭代次数: 457 损失值: 3365.2789428631754\n",
      "迭代次数: 458 损失值: 3279.506256558817\n",
      "迭代次数: 459 损失值: 3316.187247935366\n",
      "迭代次数: 460 损失值: 3469.1308521772285\n",
      "迭代次数: 461 损失值: 3310.9295465718396\n",
      "迭代次数: 462 损失值: 3283.046134616867\n",
      "迭代次数: 463 损失值: 3275.8043751571654\n",
      "迭代次数: 464 损失值: 3277.1989323180464\n",
      "迭代次数: 465 损失值: 3325.867967649267\n",
      "迭代次数: 466 损失值: 3316.5603886733247\n",
      "迭代次数: 467 损失值: 3299.266552649103\n",
      "迭代次数: 468 损失值: 3302.517830855242\n",
      "迭代次数: 469 损失值: 3279.095093953928\n",
      "迭代次数: 470 损失值: 3280.180790837583\n",
      "迭代次数: 471 损失值: 3321.6033866585776\n",
      "迭代次数: 472 损失值: 3333.273886859628\n",
      "迭代次数: 473 损失值: 3301.356683911725\n",
      "迭代次数: 474 损失值: 3264.8044946338555\n",
      "迭代次数: 475 损失值: 3399.4203131955087\n",
      "迭代次数: 476 损失值: 3346.3110095941884\n",
      "迭代次数: 477 损失值: 3322.8613510942364\n",
      "迭代次数: 478 损失值: 3290.833910049961\n",
      "迭代次数: 479 损失值: 3423.107678077965\n",
      "迭代次数: 480 损失值: 3427.152877569737\n",
      "迭代次数: 481 损失值: 3369.738843833548\n",
      "迭代次数: 482 损失值: 3314.6403216286235\n",
      "迭代次数: 483 损失值: 3293.841350942079\n",
      "迭代次数: 484 损失值: 3458.1159626110025\n",
      "迭代次数: 485 损失值: 3300.255222256923\n",
      "迭代次数: 486 损失值: 3258.6820123524517\n",
      "迭代次数: 487 损失值: 3585.080204126729\n",
      "迭代次数: 488 损失值: 3282.7866876038843\n",
      "迭代次数: 489 损失值: 3238.1865400644047\n",
      "迭代次数: 490 损失值: 3596.4000477492855\n",
      "迭代次数: 491 损失值: 3254.355587938433\n",
      "迭代次数: 492 损失值: 3217.127821185263\n",
      "迭代次数: 493 损失值: 3196.204644860869\n",
      "迭代次数: 494 损失值: 3185.994597587231\n",
      "迭代次数: 495 损失值: 3186.8488143346153\n",
      "迭代次数: 496 损失值: 3189.143896477537\n",
      "迭代次数: 497 损失值: 3210.1387473411796\n",
      "迭代次数: 498 损失值: 3198.8606506501574\n",
      "迭代次数: 499 损失值: 3192.251325696767\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实际值: [0. 1. 2. ... 8. 9. 8.] 预测值：[0. 9. 4. ... 9. 9. 9.]\n",
      "预测准确率为: 0.5147468002225932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # 计算预测准确率\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "clf = ANNClassifier(hidden_layer_sizes=(100,), eta=0.3,max_iter=500, tol=0.00001)\n",
    "clf.train(X_train_std,y_train_bin)\n",
    "y_pred_bin = clf.predict(X_test_std)\n",
    "\n",
    "# 画损失值\n",
    "n = clf.n # 迭代次数\n",
    "n_list = np.linspace(1,n,n+1)\n",
    "err_list = clf.err_list # 损失值列表\n",
    "plt.figure()\n",
    "plt.plot(n_list,err_list)\n",
    "plt.xlabel(\"迭代次数\");plt.ylabel(\"损失值\")\n",
    "plt.legend\n",
    "plt.show()\n",
    "\n",
    "y_pred = lb.inverse_transform(y_pred_bin) # 解码\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(\"实际值: %s 预测值：%s\"%(y_test,y_pred))\n",
    "print(\"预测准确率为: %s\"%(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迭代次数和学习率均有调试，但是每次损失值下降到3000就停止了，原因不明\n",
    "#### 检查了n遍网络也没有发现问题，编码解码也没问题，我怀疑是单层隐藏层的问题\n",
    "#### 预测准确率时好是坏，总体是坏"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0f97370f938c1a2e04bb69ea59b6c8fdcecf5bf4b355e2aff859ce31869b5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
